# data_engineering
This is a data engineering project that I completed for myself to get hands on spark, airflow and ETL. 

I used http://millionsongdataset.com/ dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
Also I took the log dataset consisting of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

Basically, this repository consists of two docker-compose apps: Airflow and Spark.
There is a pipeline scheduled that has two branches. One extracts raw datasets from S3 to PostgreSQL, transforms and loads data from staging tables to dimensional tables. In the end, data quality checks are run.
Another one loads data from S3 (JSON logs on user activity), processes the data into analytics tables using Spark, and loads them back into S3.

So, this repository could be an example of my skills in creating ETL pipelines. Also, feel free to fork it and/or use it for educational purposes.
